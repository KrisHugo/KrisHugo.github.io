<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>标签: AI - Blog of KrisHugo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Kris Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Kris Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Neutral, Indifference, Inertia;"><meta property="og:type" content="blog"><meta property="og:title" content="Blog of KrisHugo"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Blog of KrisHugo"><meta property="og:description" content="Neutral, Indifference, Inertia;"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Kris Hugo"><meta property="article:tag" content="Program, Writing, Diary"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Blog of KrisHugo","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Kris Hugo"},"publisher":{"@type":"Organization","name":"Blog of KrisHugo","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"Neutral, Indifference, Inertia;"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Blog of KrisHugo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">标签</a></li><li class="is-active"><a href="#" aria-current="page">AI</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-04-26T08:25:00.000Z" title="2020/4/26 下午4:25:00">2020-04-26</time>发表</span><span class="level-item"><time dateTime="2020-06-26T10:10:12.538Z" title="2020/6/26 下午6:10:12">2020-06-26</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/college/">college</a></span><span class="level-item">42 分钟读完 (大约6283个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/">深度学习专业课内容</a></h1><div class="content"><h2 id="深度学习专业课内容"><a href="#深度学习专业课内容" class="headerlink" title="深度学习专业课内容"></a>深度学习专业课内容</h2><p>吴恩达深度学习课程：<br>第一课 — 神经网络与深度学习： <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av66314465/">av66314465</a><br>第二课 — 改善深层神经网络：超参数调试、正则化以及优化：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av66524657/?spm_id_from=333.788.b_636f6d6d656e74.19">av66524657</a><br>第三课 — 结构化机器学习项目：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av66644404/">av66644404</a><br>第四课 — 卷积神经网络：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av66646276/">av66646276</a><br>第五课 — 序列模型：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av66647398/">av66647398</a></p>
<h3 id="专业课概述"><a href="#专业课概述" class="headerlink" title="专业课概述"></a>专业课概述</h3><p>AlphaGo使用的机器学习内容：蒙特卡洛树搜索，两个深度神经网络</p>
<ul>
<li>神经网络内容：<ul>
<li>卷积神经网络：ImageNet（残差网络），Inception<ul>
<li>实际应用：目标检测，人脸验证，图片风格转换</li>
</ul>
</li>
<li>序列模型：网络结构，GRU，LSTM<ul>
<li>实际应用：词嵌入，语言模型，注意力机制，触发语检测</li>
</ul>
</li>
</ul>
</li>
<li>神经网络使用领域：<ul>
<li>计算机视觉<ul>
<li>实际运用<ul>
<li>图像分类</li>
<li>语义分割</li>
<li>物体识别和检测</li>
<li>运动和跟踪</li>
<li>视觉问答</li>
<li>三维重建</li>
</ul>
</li>
</ul>
</li>
<li>自然语言处理<ul>
<li>主要是由两个内容组成<ul>
<li>自然语言理解</li>
<li>自然语言生成</li>
</ul>
</li>
<li>实际运用<ul>
<li>语音识别</li>
<li>机器翻译</li>
<li>语音合成</li>
<li>人机对话</li>
<li>语音助手</li>
<li>问答系统</li>
<li>机器阅读</li>
</ul>
</li>
</ul>
</li>
<li>生成对抗网络<ul>
<li>特质<ul>
<li>通过训练数据进行生成需要的数据</li>
<li>属于监督学习方法</li>
<li>同时训练一个生成网络和一个判别网络<ul>
<li>生成网络：生成逼真的图片欺骗判别网络</li>
<li>判别网络：区分生成图片和真实图片</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>深度学习框架<ul>
<li><strong>TensorFlow</strong> from Google for Industry<ul>
<li>19.10 -&gt; Version 2.0</li>
<li>专业课主要学习的框架，学习版本为1.x，2.0版本与1.x版本差别很大，需要分开学习<ul>
<li>编程语言：python 3.x</li>
<li>编程环境：Anaconda, Jupyter Notebook, TensorFlow 1.x</li>
</ul>
</li>
</ul>
</li>
<li><em>Keras into TensorFlow</em></li>
<li><strong>Pytorch</strong> use Python from Facebook for study</li>
<li>PaddlePaddle from Baidu</li>
<li>Deeplearning4j use JAVA </li>
<li>Mxnet from Amazon</li>
<li><em>Caffe&amp;Caffe2 into Pytorch</em></li>
<li>CNTK from Microsoft</li>
<li>Theano is ancient</li>
<li>Chainer use Python</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV164411m79z?from=search&seid=16677849041663475348">学习辅用网上视频课：吴恩达深度学习课程</a></li>
</ul>
<h2 id="第一课：视频课1-6-导论"><a href="#第一课：视频课1-6-导论" class="headerlink" title="第一课：视频课1-6 导论"></a>第一课：视频课1-6 导论</h2><ol>
<li><p>学习顺序</p>
<ol>
<li>(周1-4)Neural Networks and Deep Learning<ul>
<li>Cats Recognition</li>
</ul>
</li>
<li>Improving Deep Neural Network: Hyperparameter tuning(参数调整) , Regularization(正则化) and Optimization(优化)</li>
<li>Structing(结构化) your Machine Learning project. </li>
<li>Convolutional Neural Networks 卷积神经网络</li>
<li>Natural Language Processing.</li>
</ol>
</li>
<li><p>What is a neural network?</p>
<p><em>A function to estimate a result which need a lot of data of parameter to confirm how is the final result decided.</em></p>
<p><strong>We do not setting the method detail of solving problem</strong></p>
</li>
<li><p>Supervised Learning 监管学习</p>
<p>Give a series of Input and Gain a series of Output.</p>
<ul>
<li><p>Example:</p>
<p>​    Home features -&gt; Price (Standard NN)</p>
<p>​    Ad, user Info … -&gt; Click on ad? (Standard NN)</p>
<p>​    Image -&gt; Objects (CNN)</p>
<p>​    Audio -&gt; Text transcript (RNN【Recurrent Neural Network】)</p>
<p>​    English -&gt; Chinese (RNN)</p>
<p>​    Image, Radar Info -&gt; Position of other cars. (Custom / Hybrid)</p>
</li>
</ul>
<p><strong>Standard NN 标准神经网络，Convolutional NN 卷积神经网络，Recurrent NN 循环神经网络</strong></p>
<p><em>Neural Network Make Computer more easy to understand unstructured data.</em></p>
</li>
<li><p>Why are they just now taking off?</p>
<p>scale of data and computation is growing rapidly, and the progress in algorithms.</p>
<p><em>Using a new way (ReLu function instead of Sigmoid function) to faster the speed of training a model.</em></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/ReLu%E5%87%BD%E6%95%B0.png" alt="ReLu函数"></p>
</li>
</ol>
<h2 id="第一课：视频课7-24"><a href="#第一课：视频课7-24" class="headerlink" title="第一课：视频课7-24"></a>第一课：视频课7-24</h2><ol start="7">
<li>Binary Classification<ul>
<li><strong>A great algorithm to process the entire train sets with a great speed.</strong></li>
<li>A introduction of <strong>forward pass or forward propagation step</strong> and <strong>backward pass or backward propagation step</strong> 前向传播和反向传播</li>
<li>Logistic regression 逻辑回归<ul>
<li>An algorithm for binary classification.</li>
</ul>
</li>
<li>Important Notation <ul>
<li>‘n’ is the length of height of the matric X, ‘m’ is the count of training examples (or looks like { (x1, y1) … (x(m), y(m) ) } ). </li>
<li>there are two sets, one of them for train, another for test.</li>
<li> (Xn, Y) is a specific vector.</li>
</ul>
</li>
</ul>
</li>
<li>Logistic Regression<ul>
<li>Given x, want y hat or y^.</li>
<li>Parameters: w (matrix), b (real number)</li>
<li>Output: y^ = w^T * x + b (Sigmoid function) = Sigmoid(Z)<ul>
<li> ![sigmoid function](./深度学习专业课内容/Sigmoid function.png “sigmoid function”)</li>
<li><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/Sigmoid(Z).png" alt="Sigmoid(Z)"></li>
<li><em>learn about w and b to know how y become a good estimate.</em></li>
<li><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/MatrixOfResult.png" alt="Another academic method of display" title="Another academic method of display which will be not used in this course"></li>
</ul>
</li>
</ul>
</li>
<li>Logistic Regression cost function</li>
</ol>
<p><strong>To train the parameters W and B of the logistic regression model, We need to define a cost function</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/yHat.png" alt="yHat"></p>
<p>To get the best prediction, we want the y Hat can be more and more near to the i-th example.</p>
<p><strong>Lost funcion</strong> is used to measure how well our algorithm is doing.</p>
<p>Here is the not good algorithm: (which will get serveral global optimums.)</p>
<p>![Old Loss function](./深度学习专业课内容/Old Loss function.png “Old Loss function”)</p>
<p>Here is the better algorithm: (of which graph is convex)</p>
<p>![New Loss function](./深度学习专业课内容/Loss function.png “New Loss Function”)</p>
<p><strong>Cost function:</strong> the cost of the parameters.</p>
<p>![Cost function](./深度学习专业课内容/Cost function.png “Cost function”)</p>
<p>Compute the total cost of the whole sets, Measure how well the performance of your trained model.</p>
<ol start="10">
<li>Gradient Descent 梯度下降法</li>
</ol>
<p><strong>The method to train the w and b.</strong></p>
<p><em>ways: Find the minimum or to say the global optimum.</em></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/GradientDescent.png" alt="Cost Function"></p>
<p>The real usage of Gradient Descent.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/GradientDescentAlgorithm.png" alt="GradientDescent"></p>
<p>Parameters in this function:    </p>
<p>​    α ：learning rate ：which to control how big a step we take on each iteration or gradient descent.</p>
<p>​    <strong>w := means updated w equal to the right</strong> </p>
<p><strong>The Key of function</strong></p>
<ul>
<li><strong>using derivatives导数 to find the side which is more near to the optimum</strong></li>
<li>and update the nearest value and repeat it until you can not fould the next value.</li>
</ul>
<ol start="11">
<li>derivatives 导数</li>
</ol>
<p>略（PS:作为一个正在考研的人，要是这里还需要这种入门级的教学，我就别考了hhh）</p>
<ol start="13">
<li>Computation Graph 计算图</li>
</ol>
<p><strong>Also the introduction about derivaives, but the graph makes the procedure of whole computation a lot easier to be comprehended.</strong></p>
<p><em>Example:</em></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/ComputationGraph.png" alt="ComputationGraph"></p>
<p><em>The Compute of Derivatives in Computation Graph</em></p>
<p>![image-20200430115619173](./深度学习专业课内容/The backward propagation.png)</p>
<ol start="15">
<li>Logistic Regression - Gradient descent 逻辑回归中的梯度下降法</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/Blog\KrisHugo.github.io\source_posts\深度学习专业课内容\LogisticRegressionRecap.png" alt="LogisticRegressionRecap"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/LogisticRegressionDerivatives.png" alt="LogisticRegressionDerivatives"></p>
<p>And Find the derivatives for L : </p>
<p><strong>da = dL / da = - y / a + (1 - y) / (1 - a)</strong></p>
<p><strong>dz = dL / dz =  dL / da * da / dz = dL / da * a(1-a) = a - y</strong></p>
<p>**dw1 = dL / dw1 = x1 * dz = x1 * (a - y) **</p>
<p>**dw2 = x2 * dz **</p>
<p><strong>db = dz</strong></p>
<h4 id="using-Gradient-descent-on-M-samples"><a href="#using-Gradient-descent-on-M-samples" class="headerlink" title="using Gradient descent on M samples"></a>using Gradient descent on M samples</h4><p>The used function is <strong>J(w, b)</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/LogisticRegressionOnMExamples.png" alt="LogisticRegressionOnMExamples"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/CalculateTheLossFunction.png" alt="CalculateTheLossFunction"></p>
<ol start="17">
<li>Vetorization 向量化</li>
</ol>
<p><strong>A great important method to forbid repeatitive and inefficient looping</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/ExampleForVectorizedSpeed.png" alt="ExampleForVectorizedSpeed"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/Numpy.png" alt="A more faster way of calculate a series of great datasets"></p>
<ol start="19">
<li>Vectorizing Logistic Regression 向量化逻辑回归</li>
</ol>
<p><strong>Using Python Module – Numpy to Simplify the Code</strong></p>
<ol start="20">
<li>Vectorizing Logistic Regresion’s Gradient Computation 向量化逻辑回归的梯度计算</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/ImplementingLogisticRegression.png" alt="ImplementingLogisticRegression"></p>
<ol start="21">
<li>Broadcasting in Python</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/Broadingcasting.png" alt="Broadingcasting"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/ExampleForBroadcasting.png" alt="ExampleForBroadcasting"></p>
<h2 id="第一课：视频课25-35"><a href="#第一课：视频课25-35" class="headerlink" title="第一课：视频课25-35"></a>第一课：视频课25-35</h2><ol start="25">
<li><p>Neural Network Overview</p>
</li>
<li><p>Neural Network Representation</p>
</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200515110302243.png" alt="Neural Network Representation"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200515110648704.png" alt="image-20200515110648704"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200515111536381.png" alt="image-20200515111536381"></p>
<ol start="28">
<li>Vertorizing</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200515112005402.png" alt="image-20200515112005402"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200515112240179.png" alt="image-20200515112240179"></p>
<ol start="30">
<li>Activation functions</li>
</ol>
<p>Here are four useful and often activation function:</p>
<ul>
<li>sigmoid : It doesn’t to be used any more, because it has some disadventages</li>
<li>tanh: limit the output between 0 and 1</li>
<li>ReLU: Rectified limit Unit – a = max(0, 1) <strong>(Mr.Wu recommend)</strong></li>
<li>Leaky ReLU: Rectified limit Unit which is not zero when input go down zero.</li>
</ul>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200515114039515.png" alt="image-20200515114039515"></p>
<ol start="31">
<li>Why we use non-linear activate function.</li>
</ol>
<p>If we use linear activate function in neural network, the whole hidden layers is useless, and it may be OK when you wanna computing output a real number.**So we only use linear function in the final output layer, except for some special circumstance. **but In that case, using ReLU is fine, too</p>
<ol start="32">
<li>the deriviatives of Activate functions</li>
</ol>
<ul>
<li>sigmoid -&gt; a’ = sigmoid’(z) =  a (1 - a)</li>
<li>tanh -&gt; a’ = tanh’(z) = 1 - a^2</li>
<li>ReLU -&gt; a’ = (max(0, z))’ = 0, if z &lt; 0; 1, if z &gt;= 0.</li>
<li>Leaky ReLU -&gt; a’ = (max(0.01 * z, z))’ = 0.01, if z &lt; 0; 1 if z &gt;=0.</li>
</ul>
<ol start="33">
<li>Gradient descent for neural networks</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200515120536239.png" alt="image-20200515120536239"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200515121112957.png" alt="backward propagation"></p>
<ol start="35">
<li>Randomly Initialization</li>
</ol>
<p> If we don’t randomly initialize paremeter W, we will get the same result from each unit in one </p>
<p>layer. It will make out gradient descent to doesn’t work by makes the units output the same results.</p>
<p><strong>b initialized by zeros is OK, but W should be randomly initialized.</strong></p>
<p>In Python: </p>
<ul>
<li><p>w1 = np.random.rand((a, b)) * 0.01 (<em>why here is 0.01 instead of 100 or 1000?</em>)</p>
<ul>
<li><strong>the latter multiplier usually be very small because if this parameter become too large, it makes the sigmoid function or tanh or others useless, hence it makes the learning too slow.</strong></li>
</ul>
</li>
<li><p>b1 = np.zero((b, 1))</p>
</li>
</ul>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200518213524372.png" alt="image-20200518213524372"></p>
<h2 id="第一课：视频课36-43"><a href="#第一课：视频课36-43" class="headerlink" title="第一课：视频课36-43"></a>第一课：视频课36-43</h2><ol start="36">
<li>Deep L-layer Neural network</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522101551302.png" alt="image-20200522101551302"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522101958747.png" alt="image-20200522101958747"></p>
<ol start="38">
<li>getting your matrix dimensions right</li>
</ol>
<p>单变量的情况下：</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522104705786.png" alt="image-20200522104705786"></p>
<p>Z[l] -&gt; (n[l], 1); A[I] -&gt;  (n[l], 1);</p>
<p>向量化后：<strong>when function plus parameter b, python will automatically using broadcast to duplicate b into a (n, m) matrix</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522105131223.png" alt="image-20200522105131223"></p>
<ol start="39">
<li>Why deep representations?</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522110207245.png" alt="image-20200522110207245"></p>
<ol start="40">
<li>Building blocks of deep neural networks</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522111438793.png" alt="image-20200522111438793"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522111720050.png" alt="image-20200522111720050"></p>
<ol start="37">
<li>Forward propagation in a deep network</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522102625960.png" alt="image-20200522102625960"></p>
<ol start="41">
<li>Forward and backward propagation</li>
</ol>
<p><strong>Forward propagation:</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522112104484.png" alt="image-20200522112104484"></p>
<p><strong>Backward propagation:</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522112407294.png" alt="image-20200522112407294"></p>
<p><strong>Noticing the backward propagation initialization parameter ==dA==</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200522112701454.png" alt="image-20200522112701454"></p>
<ol start="42">
<li>Parameters ==vs== Hyperparameters</li>
</ol>
<p>Hyperparameters is controlled by researchers, which could directly effect the paramters ==W, B==;</p>
<h2 id="第二课：视频课1-14"><a href="#第二课：视频课1-14" class="headerlink" title="第二课：视频课1-14"></a>第二课：视频课1-14</h2><ol>
<li>Train / dev / test sets</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525092506646.png" alt="image-20200525092506646"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525093136620.png" alt="image-20200525093136620"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525093421911.png" alt="image-20200525093421911"></p>
<ol start="2">
<li>Bias / Variance 偏差/方差</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525093718667.png" alt="image-20200525093718667"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525094113692.png" alt="image-20200525094113692"></p>
<ol start="3">
<li>Basic recipe for machine learning</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525095149437.png" alt="image-20200525095149437"></p>
<ol start="4">
<li>Regularization 正则化</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525095646075.png" alt="image-20200525095646075"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525100202996.png" alt="image-20200525100202996"></p>
<ol start="5">
<li>Why regularization reduces overfittings?</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525101523571.png" alt="image-20200525101523571"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525101056476.png" alt="image-20200525101056476"></p>
<ol start="6">
<li>Dropout regularization 随机失活正则化</li>
</ol>
<p>Randomly choose several units in each layer and set them disabled</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525102156007.png" alt="image-20200525102156007"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525103248930.png" alt="image-20200525103248930"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525103527601.png" alt="image-20200525103527601"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525104132554.png" alt="image-20200525104132554"></p>
<ol start="8">
<li>Other regularization methods</li>
</ol>
<p>Data augmentation (when we can’t get more data)</p>
<p>flipping horizontally, rotated or distortion</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525105156574.png" alt="image-20200525105156574"></p>
<p>Early stopping (avoid overfitting)</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525105734977.png" alt="image-20200525105734977"></p>
<ol start="9">
<li>Normalizing inputs</li>
</ol>
<p>There are two steps:</p>
<ul>
<li>subtract mean (zero mean)</li>
<li>normalize variance </li>
</ul>
<p><strong>Use the same mu and sigma in train set and test set</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525110214245.png" alt="image-20200525110214245"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525110622485.png" alt="image-20200525110622485"></p>
<ol start="10">
<li>Vanishing / exploding gradients</li>
</ol>
<p>If each W greater than 1, the activation will be exploding.</p>
<p>If each W smaller than 1, the activation will be vanishing.</p>
<p>They will also make the learning very slow.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525111309317.png" alt="image-20200525111309317"></p>
<ol start="11">
<li>Weight initialization for deep network</li>
</ol>
<p>Because of the existence of Vanishing / exploding gradients, we have to take careful in weight initialization.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525112033756.png" alt="image-20200525112033756"></p>
<ol start="12">
<li>Gradient checking</li>
</ol>
<p><strong>For checking if the compute of derivatives is correct.</strong></p>
<p> Numerical approximation of gradients 梯度数值逼近</p>
<p>Using the double sides derivative, which makes the Error much less than one side derivative.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525113200276.png" alt="image-20200525113200276"></p>
<p>Gradient checking</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525113515862.png" alt="image-20200525113515862"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200525113823819.png" alt="image-20200525113823819"></p>
<ol start="14">
<li>Gradient Checking implementation notes.</li>
</ol>
<ul>
<li>Don’t use in training, only to debug.</li>
<li>If algorithm fails grad check, look at components to try to identify bug.</li>
<li>Remember regularization.</li>
<li>Doesn’t work with dropout.</li>
<li>Run at ramdom initialization; perhaps again after some training.</li>
</ul>
<h2 id="第二课：视频课15-24"><a href="#第二课：视频课15-24" class="headerlink" title="第二课：视频课15-24"></a>第二课：视频课15-24</h2><ol start="15">
<li>Mini-batch gradent descent</li>
</ol>
<p> When <strong>the count of sample</strong> is too large, the speed of using your algorithm will still be too slow to compute. </p>
<p>Mini-batch means a little sets of trainning samples extracted from a whole tremendous training samples sets.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200530164024668.png" alt="image-20200530164024668"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200530164538205.png" alt="image-20200530164538205"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200530164819579.png" alt="image-20200530164819579"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200530171013466.png" alt="image-20200530171013466"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200530171241167.png" alt="image-20200530171241167"></p>
<ol start="17">
<li>Exponentially weighted averages. 指数加权平衡</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200530204422817.png" alt="image-20200530204422817"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531095044302.png" alt="image-20200531095044302"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531095701942.png" alt="image-20200531095701942"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531095919807.png" alt="image-20200531095919807"></p>
<p>The adventages of exponentially weighted averages:</p>
<ul>
<li> it just takes up one line of code basically. 只占用一行代码 </li>
<li>and just storage and memory for a single row number to compute this exponentially weighted average. 只需要存储单行数字内容</li>
<li>not the best and most accurate way to compute average.</li>
</ul>
<ol start="19">
<li>Bias correction in exponentially weighted averages</li>
</ol>
<p>In last example, using exponentially weighted averages might make the first several averages too low to be accurate to portray the first several days temperature.</p>
<p>So, we have to use bias to fix it.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531100714813.png" alt="image-20200531100714813"></p>
<p>The bias will go very near to zero when days go up, so we don’t worry about the influence of bias in rear samples.</p>
<p><strong>Most people don’t care about it unless you have to concentrate on the first several indics</strong></p>
<ol start="20">
<li>Gradient descent with momentum 动量梯度下降法</li>
</ol>
<p>In previous methods, we have to make the learning rate low lest make the predictions go far away from the minimum.  </p>
<p>In Gradient descent with momentum, we using the exponentially weighted averages as our forward momentum to keep the learning orientation is point to the position of the minimum.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531101841756.png" alt="image-20200531101841756"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531102131122.png" alt="image-20200531102131122"></p>
<ol start="21">
<li>RMSprop </li>
</ol>
<p>Another way to speed up gradient descent.</p>
<p><em>I do not really comprehend understand it.</em></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531103133043.png" alt="image-20200531103133043"></p>
<ol start="22">
<li>Adam optimization algorithm</li>
</ol>
<p>Adam : Adaption Moment Estimation</p>
<p>Another way to speed up gradient descent using in generally</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531105317686.png" alt="image-20200531105317686"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531105418927.png" alt="image-20200531105418927"></p>
<ol start="23">
<li>Learning rate decay</li>
</ol>
<p>There are several way to make the learning rate goes down with the iteration goes through.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531105938019.png" alt="image-20200531105938019"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531110051578.png" alt="image-20200531110051578"></p>
<ol start="24">
<li>The problem of local optima 局部最优问题</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531110604893.png" alt="image-20200531110604893"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200531110724616.png" alt="image-20200531110724616"></p>
<p><strong>The optimization methods could help us exemplify from the local optima</strong></p>
<h2 id="第二课：视频课25-35"><a href="#第二课：视频课25-35" class="headerlink" title="第二课：视频课25-35"></a>第二课：视频课25-35</h2><ol start="25">
<li>Tuning process 调试处理</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601204302879.png" alt="image-20200601204302879"></p>
<p>Find the best hyperparameters‘ values</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601204634913.png" alt="image-20200601204634913"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601204808739.png" alt="image-20200601204808739"></p>
<ol start="26">
<li>Using an appropriate scale to pick hyperparameters</li>
</ol>
<p>A normal way to choose but not for all.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601205022283.png" alt="image-20200601205022283"></p>
<p>A special way for estimate the hyperparameters which is sensitive for a very small changes. <img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601205248898.png" alt="image-20200601205248898"></p>
<p>Because of the trait of exponentially weighted averages, which is when it comes too close to 1,  the influence of it will become more and more intensive. </p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601205745768.png" alt="image-20200601205745768"></p>
<ol start="27">
<li> Hyperparameters tuning in practice: Pandas vs. Caviar</li>
</ol>
<p>The two different way of hyperparameters tuning.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601210803581.png" alt="image-20200601210803581"></p>
<ol start="28">
<li>Normalizing activations in a network.</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601211313319.png" alt="image-20200601211313319"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601211758052.png" alt="image-20200601211758052"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601214817911.png" alt="image-20200601214817911"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200601215137626.png" alt="image-20200601215137626"></p>
<ol start="30">
<li>Why Batch Norm work?</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603111555383.png" alt="image-20200603111555383"></p>
<p>the function of Batch Norm is to control the mean (to zero) and variance (to one) of the input of each hidden layers by transfer the shape of input to normal distribution.</p>
<p>so why we have to control the mean and variance?</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603112741076.png" alt="image-20200603112741076"></p>
<p>this controlling is to undermine covariate shift which means the precede hidden layers’ parameters updated could put a great influence into the output, affect the latter hidden layers’ parameters, which means in chinese “牵一发而动全身”. It makes the updating of neural networks is unstable and chaos, which I mean when our newest data have a great difference from prior data, the neural network might perform worse.</p>
<p>but using inputs with the shape of normal distribution could avert it.</p>
<p><strong>So the truly meaning of using batch norm is to make each hidden layer more independent.</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603113317734.png" alt="image-20200603113317734"></p>
<ol start="31">
<li>Batch norm at test time</li>
</ol>
<p>In test time, we need other way to calculate mu and sigma</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603114739137.png" alt="image-20200603114739137"></p>
<ol start="32">
<li>Softmax regression</li>
</ol>
<p>one of generalizations of logical regression</p>
<p>Not just binary classification, but could give a variety of result.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603115657782.png" alt="image-20200603115657782"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603120159683.png" alt="image-20200603120159683"></p>
<p><strong>The different is input a vector, output a vector</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603120441817.png" alt="image-20200603120441817"></p>
<ol start="33">
<li>Training a softmax classification.</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603120759334.png" alt="image-20200603120759334"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603121403079.png" alt="image-20200603121403079"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603121622370.png" alt="image-20200603121622370"></p>
<ol start="34">
<li>Deep Learning frameworks</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603121814542.png" alt="image-20200603121814542"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603121851611.png" alt="image-20200603121851611"></p>
<ol start="35">
<li>TensorFlow</li>
</ol>
<ul>
<li>Basically using</li>
</ul>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603122748753.png" alt="image-20200603122748753"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603122903528.png" alt="image-20200603122903528"></p>
<ul>
<li>input data</li>
</ul>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603123148606.png" alt="image-20200603123148606"></p>
<ul>
<li>whole code example</li>
</ul>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200603123435243.png" alt="image-20200603123435243"></p>
<h2 id="第三课：视频课1-12"><a href="#第三课：视频课1-12" class="headerlink" title="第三课：视频课1-12"></a>第三课：视频课1-12</h2><ol>
<li>Why is ML strategy</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200606091453068.png" alt="image-20200606091453068"></p>
<p>How to choose the best method for improve the performance of ML.</p>
<ol start="2">
<li>Orthogonalization 正交化</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200606092046899.png" alt="image-20200606092046899"></p>
<p>Orthogonalization means let each parameter when it changes will not affect other parameters.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200606092742211.png" alt="image-20200606092742211"></p>
<ol start="3">
<li>Single number evaluation metric 单一数字评估指标</li>
</ol>
<p>Precision 查准率 of the example that your classifier recognizes as cats, what percentage actually are cats?</p>
<p>Recall 查全率 of all the images that really are cats, what percentage were correctly recognized by your classifier?</p>
<p>F1 Score  <strong>the mixture of P and R</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200606102433184.png" alt="image-20200606102433184"></p>
<p>By using the average, it make us easy to figure out which classifier is performing better than others classifier overall.</p>
<ol start="4">
<li>Satisficing and optimizing metrics.</li>
</ol>
<p>We should care about both accuracy and running time.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200606103351231.png" alt="image-20200606103351231"></p>
<ol start="5">
<li>Train/dev/test distributions</li>
</ol>
<p><strong>One bad idea: using not really randomly choosed sets for equally distributing.</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200610103534117.png" alt="image-20200610103534117"></p>
<p>==A real great idea is to set the dev and test sets both have data containing all apperent conditions such as region in this example.==</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200610103903800.png" alt="image-20200610103903800"></p>
<ol start="6">
<li>Size of dev and test sets</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200610104151103.png" alt="image-20200610104151103"></p>
<p><strong>But in the era of big data, we have to change ways to decide the size of dev and test sets</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200610104248502.png" alt="image-20200610104248502"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200610104500075.png" alt="image-20200610104500075"></p>
<ol start="7">
<li>When to change dev/test sets and metrics</li>
</ol>
<p><strong>Sometimes, one algorithm maybe perform greater than others, but it will output some unacceptible results when it goes wrong. So we may not use it because of terrible consequence because of the algorithm’s low-possible but inexorable blunder.</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200610105102600.png" alt="image-20200610105102600"></p>
<p><strong>Here are our usage which weighting the index of each error emerging to make us get the best algorithm which fits our individually special demand</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200610110225404.png" alt="image-20200610110225404"></p>
<ol start="8">
<li>Why human-level perormance</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200610110759027.png" alt="image-20200610110759027"></p>
<p>Bayes optimal error, or Bayesian optimal error, or Bayes error for short is the very best theoretical function for mapping from x to y. </p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200610111146468.png" alt="image-20200610111146468"></p>
<ol start="9">
<li>Avoidable bias</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200611115346592.png" alt="image-20200611115346592"></p>
<ol start="10">
<li>Understanding human-level performance</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200613091305773.png" alt="image-20200613091305773"></p>
<p><strong>How should you define human-level error?</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200613092311029.png" alt="image-20200613092311029"></p>
<p>**Of course we should use the best performance as human-level error, to substitute or estimate the Bayes error, but it this case, when our AI do not perform good enough, choose using anyone to be Bayes error is meaningless. **</p>
<p><strong>When avoidable bias is non-ignorable, we should keep eyes on  Bias, otherwise we should pay attention to Variance.</strong></p>
<ol start="11">
<li>Surpassing human-level performance</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200613093244367.png" alt="image-20200613093244367"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200613093611006.png" alt="image-20200613093611006"></p>
<ol start="12">
<li>Improving your model performance</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200613094016023.png" alt="image-20200613094016023"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200613094305142.png" alt="image-20200613094305142"></p>
<h2 id="第三课：视频课13-22"><a href="#第三课：视频课13-22" class="headerlink" title="第三课：视频课13-22"></a>第三课：视频课13-22</h2><ol start="13">
<li>Carring out error analysis 错误分析</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615144412091.png" alt="image-20200615144412091"></p>
<ol>
<li>Found how the error may be caused.</li>
<li>Figure out the possible reasons.</li>
<li>Use a sheet to statistic.</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615145025091.png" alt="image-20200615145025091"></p>
<ol start="14">
<li>cleaning up incorrectly labeled data</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615145422040.png" alt="image-20200615145422040"></p>
<p>So long as the total data set is big enough, the random error is ok, but the systematic errors is not ok.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615150053960.png" alt="image-20200615150053960"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615150516455.png" alt="image-20200615150516455"></p>
<ol start="15">
<li>Build your first system quickly, then iterate</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615151036893.png" alt="image-20200615151036893"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615151140380.png" alt="image-20200615151140380"></p>
<p><strong>Guideline: Build a system first, and iterate.</strong></p>
<ol start="16">
<li>Training and testing on different distributions.</li>
</ol>
<p>There are two ways to solve it.</p>
<ol>
<li>Merge them and shuffle.</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615153559584.png" alt="image-20200615153559584"></p>
<ol start="2">
<li>use basis dataset to train, and the real worthy data to test.</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615154153186.png" alt="image-20200615154153186"></p>
<ol start="17">
<li>Bias and Variance with mismatched data distributions</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615160321279.png" alt="image-20200615160321279"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615160644471.png" alt="image-20200615160644471"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615161232388.png" alt="image-20200615161232388"></p>
<ol start="18">
<li>Addressing data mismatch</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615161755207.png" alt="image-20200615161755207"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615161946845.png" alt="image-20200615161946845"></p>
<ol start="19">
<li>Transfer learning 迁移学习</li>
</ol>
<p>Use old knowledge in new task</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615163710699.png" alt="image-20200615163710699"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615164002861.png" alt="image-20200615164002861"></p>
<ol start="20">
<li>Multi-task learning 多任务学习</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615164203212.png" alt="image-20200615164203212"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615164535439.png" alt="image-20200615164535439"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615164843129.png" alt="image-20200615164843129"></p>
<ol start="21">
<li>End-to-End deep learning 端对端深度学习</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615165235342.png" alt="image-20200615165235342"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615165819492.png" alt="image-20200615165819492"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615165939240.png" alt="image-20200615165939240"></p>
<ol start="22">
<li>Whether to use end-to-end deep learning</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615170349079.png" alt="image-20200615170349079"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200615170925241.png" alt="image-20200615170925241"></p>
<h2 id="第四课：视频课1-11"><a href="#第四课：视频课1-11" class="headerlink" title="第四课：视频课1-11"></a>第四课：视频课1-11</h2><ol>
<li>Computer Vision</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616084951453.png" alt="image-20200616084951453"></p>
<ol start="2">
<li>Edge Detection</li>
</ol>
<p><strong>Convolution 卷积</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616085511773.png" alt="image-20200616085511773"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616085604059.png" alt="image-20200616085604059"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616085655347.png" alt="image-20200616085655347"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616085806762.png" alt="image-20200616085806762"></p>
<p>The usage of convolution in Vertical edge detection</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616090001930.png" alt="image-20200616090001930"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616090644499.png" alt="image-20200616090644499"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616090731360.png" alt="image-20200616090731360"></p>
<ol start="4">
<li>Padding</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616091338859.png" alt="image-20200616091338859"></p>
<p><strong>Two strategy of Padding convolutions</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616091512938.png" alt="image-20200616091512938"></p>
<p><strong>We usually use odd-numbered f for building filter</strong></p>
<ol start="5">
<li>Strided convolution</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616091748865.png" alt="image-20200616091748865"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616091757404.png" alt="image-20200616091757404"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616091814573.png" alt="image-20200616091814573"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616091925121.png" alt="image-20200616091925121"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616092444984.png" alt="image-20200616092444984"></p>
<ol start="6">
<li>Convolution over volumes</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616092639613.png" alt="image-20200616092639613"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616092811448.png" alt="image-20200616092811448"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616093039875.png" alt="image-20200616093039875"></p>
<ol start="7">
<li>One layer of convolutional neural network</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616093610946.png" alt="image-20200616093610946"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616093711561.png" alt="image-20200616093711561"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616094232888.png" alt="image-20200616094232888"></p>
<p><strong>An Example of ConvNet</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616095041404.png" alt="image-20200616095041404"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616095216286.png" alt="image-20200616095216286"></p>
<ol start="9">
<li>Pooling layers 池化层（汇合层）</li>
</ol>
<p><strong>Use Pooling layers to reduce the size of their representation to speed up computtion, as wel as to some of the features it detects a bit more rebust. 缩减模型大小，提高计算速度，同时提高所提取特征的健壮性</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616095619531.png" alt="image-20200616095619531"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616095802291.png" alt="image-20200616095802291"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616100019095.png" alt="image-20200616100019095"></p>
<ol start="10">
<li>Neural network example</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616100952112.png" alt="image-20200616100952112"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616101006120.png" alt="image-20200616101006120"></p>
<ol start="11">
<li>Why convolution?</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616101443682.png" alt="image-20200616101443682"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616101630163.png" alt="image-20200616101630163"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200616101947696.png" alt="image-20200616101947696"></p>
<h3 id="第四课：视频课12-22"><a href="#第四课：视频课12-22" class="headerlink" title="第四课：视频课12-22"></a>第四课：视频课12-22</h3><ol start="12">
<li>Why look at cases studies?</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200622195228815.png" alt="image-20200622195228815"></p>
<ol start="13">
<li>Classic networks</li>
</ol>
<ul>
<li><strong>LeNet - 5 : Recognize the number written by hand</strong></li>
</ul>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200622195928733.png" alt="image-20200622195928733"></p>
<ul>
<li>Alex Net</li>
</ul>
<p>Input a picture, return a series.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200622200326093.png" alt="image-20200622200326093"></p>
<ul>
<li>VGG - 16</li>
</ul>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626102816754.png" alt="image-20200626102816754"></p>
<ol start="14">
<li>Residual Network — Res Net</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626103551402.png" alt="image-20200626103551402"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626103751284.png" alt="image-20200626103751284"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626104551750.png" alt="image-20200626104551750"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626104906027.png" alt="image-20200626104906027"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626105033227.png" alt="image-20200626105033227"></p>
<ol start="16">
<li>Network in Network and 1 * 1 convolutions</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626105530885.png" alt="image-20200626105530885"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626105656715.png" alt="image-20200626105656715"></p>
<ol start="17">
<li>Inception Network Motivation</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626110040713.png" alt="image-20200626110040713"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626110247040.png" alt="image-20200626110247040"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626110517819.png" alt="image-20200626110517819"></p>
<p>1*1 convolution use to reduce the size of third layers</p>
<ol start="18">
<li>Inception Network</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626110818596.png" alt="image-20200626110818596"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626111002896.png" alt="image-20200626111002896"></p>
<ol start="19">
<li>Using open-source implementations</li>
</ol>
<p>Github</p>
<ol start="20">
<li>Transfer Learning</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626111551586.png" alt="image-20200626111551586"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626111711592.png" alt="image-20200626111711592"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626111812124.png" alt="image-20200626111812124"></p>
<ol start="21">
<li>Data augmentation</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626112129330.png" alt="image-20200626112129330"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626112227329.png" alt="image-20200626112227329"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626112411712.png" alt="image-20200626112411712"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626112530514.png" alt="image-20200626112530514"></p>
<ol start="22">
<li>The state of computer vision</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626112849086.png" alt="image-20200626112849086"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626113335320.png" alt="image-20200626113335320"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626113601239.png" alt="image-20200626113601239"></p>
<h3 id="第四课-视频课22-32"><a href="#第四课-视频课22-32" class="headerlink" title="第四课 视频课22-32"></a>第四课 视频课22-32</h3><ol start="22">
<li>Object localization</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626130754143.png" alt="image-20200626130754143"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626131613768.png" alt="image-20200626131613768"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626131924761.png" alt="image-20200626131924761"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626132129076.png" alt="image-20200626132129076"></p>
<ol start="23">
<li>Landmark detection</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626133150844.png" alt="image-20200626133150844"></p>
<ol start="24">
<li>Object detection</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626134703112.png" alt="image-20200626134703112"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626134920442.png" alt="image-20200626134920442"></p>
<ol start="25">
<li>Convolutional implementation of sliding windows</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626135530490.png" alt="image-20200626135530490"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626141058696.png" alt="image-20200626141058696"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626141205599.png" alt="image-20200626141205599"></p>
<ol start="27">
<li>Bouding box prediction</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626141323362.png" alt="image-20200626141323362"></p>
<p><strong>YOLO algorithm</strong></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626170208739.png" alt="image-20200626170208739"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626170233582.png" alt="image-20200626170233582"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626170446787.png" alt="image-20200626170446787"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626171014053.png" alt="image-20200626171014053"></p>
<ol start="28">
<li>Intersection over union 交并比</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626171327450.png" alt="image-20200626171327450"></p>
<p><strong>Is it good enough or too bad?</strong></p>
<p>We have to use a algorithm to evaluate it.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626171519270.png" alt="image-20200626171519270"></p>
<ol start="29">
<li>Non-max suppresion 非极大值抑制</li>
</ol>
<p>One result only be detected once.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626172514450.png" alt="image-20200626172514450"></p>
<ol start="30">
<li>Anchor Boxes</li>
</ol>
<p>Detect multiple objects.</p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626173155401.png" alt="image-20200626173155401"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626173254454.png" alt="image-20200626173254454"><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626173556907.png" alt="image-20200626173556907"></p>
<ol start="31">
<li>YOLO algorithm</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626174008319.png" alt="image-20200626174008319"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626174316780.png" alt="image-20200626174316780"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626174513394.png" alt="image-20200626174513394"></p>
<ol start="32">
<li>Region proposals 候选区域</li>
</ol>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626175039025.png" alt="image-20200626175039025"></p>
<p><img src="/2020/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%86%85%E5%AE%B9/image-20200626180816656.png" alt="image-20200626180816656"></p>
<h3 id="第四课：视频课33-43"><a href="#第四课：视频课33-43" class="headerlink" title="第四课：视频课33-43"></a>第四课：视频课33-43</h3></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Kris Hugo"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Kris Hugo</p><p class="is-size-6 is-block">An alive</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Nowhere</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">36</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">27</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">46</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/KrisHugo" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/KrisHugo"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">九月 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/07/"><span class="level-start"><span class="level-item">七月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">六月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">五月 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/04/"><span class="level-start"><span class="level-item">四月 2020</span></span><span class="level-end"><span class="level-item tag">21</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">二月 2020</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AppServ/"><span class="tag">AppServ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/English/"><span class="tag">English</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lite-Travel/"><span class="tag">Lite-Travel</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Philosophy/"><span class="tag">Philosophy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Program/"><span class="tag">Program</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Shell/"><span class="tag">Shell</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Translation/"><span class="tag">Translation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/big-data/"><span class="tag">big data</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-analyze/"><span class="tag">data analyze</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/diary/"><span class="tag">diary</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/elastic-search/"><span class="tag">elastic-search</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/java/"><span class="tag">java</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kibana/"><span class="tag">kibana</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/log/"><span class="tag">log</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mysql/"><span class="tag">mysql</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/phpMyAdmin/"><span class="tag">phpMyAdmin</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/program/"><span class="tag">program</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/test/"><span class="tag">test</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/themes/"><span class="tag">themes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/writing/"><span class="tag">writing</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%87%86%E5%A4%87/"><span class="tag">准备</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%95%86%E5%BF%97/"><span class="tag">商志</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/"><span class="tag">基本知识</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"><span class="tag">基础学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><span class="tag">基础知识</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AE%9E%E6%93%8D/"><span class="tag">实操</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B0%8F%E4%BD%9C%E6%96%87/"><span class="tag">小作文</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/"><span class="tag">微观经济学</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%84%9F%E6%83%B3/"><span class="tag">感想</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%94%BF%E6%B2%BB/"><span class="tag">政治</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6%E4%B8%80/"><span class="tag">数学一</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E5%85%AC/"><span class="tag">考公</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E7%A0%94/"><span class="tag">考研</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%8B%B1%E8%AF%AD/"><span class="tag">英语</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%8D%E6%B1%87/"><span class="tag">词汇</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%BE%E5%A4%96%E5%AD%A6%E4%B9%A0/"><span class="tag">课外学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%97%AE%E9%A2%98/"><span class="tag">问题</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95/"><span class="tag">面试</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/"><span class="tag">高等数学</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AppServ/"><span class="level-start"><span class="level-item">AppServ</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/AppServ/phpMyAdmin/"><span class="level-start"><span class="level-item">phpMyAdmin</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/C/Sort/"><span class="level-start"><span class="level-item">Sort</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/English/"><span class="level-start"><span class="level-item">English</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Lite-Travel/"><span class="level-start"><span class="level-item">Lite-Travel</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/college/"><span class="level-start"><span class="level-item">college</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/diary/"><span class="level-start"><span class="level-item">diary</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/elastic-search/"><span class="level-start"><span class="level-item">elastic-search</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/hexo/"><span class="level-start"><span class="level-item">hexo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/hexo/themes/"><span class="level-start"><span class="level-item">themes</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/hexo/themes/program/"><span class="level-start"><span class="level-item">program</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/java/"><span class="level-start"><span class="level-item">java</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/linux/"><span class="level-start"><span class="level-item">linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/mysql/"><span class="level-start"><span class="level-item">mysql</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/test/"><span class="level-start"><span class="level-item">test</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/writing/"><span class="level-start"><span class="level-item">writing</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/"><span class="level-start"><span class="level-item">微观经济学</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E5%85%AC/"><span class="level-start"><span class="level-item">考公</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94/"><span class="level-start"><span class="level-item">考研</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94/English/"><span class="level-start"><span class="level-item">English</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94/%E6%94%BF%E6%B2%BB/"><span class="level-start"><span class="level-item">政治</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94/%E6%95%B0%E5%AD%A6/"><span class="level-start"><span class="level-item">数学</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94/%E8%8B%B1%E8%AF%AD/"><span class="level-start"><span class="level-item">英语</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E9%97%AE%E9%A2%98/"><span class="level-start"><span class="level-item">问题</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E9%9D%A2%E8%AF%95/"><span class="level-start"><span class="level-item">面试</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-14T09:25:42.000Z">2021-09-14</time></p><p class="title"><a href="/2021/09/14/test/">test</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-13T13:25:27.708Z">2021-09-13</time></p><p class="title"><a href="/2021/09/13/QuickSort-%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/">QuickSort-快速排序</a></p><p class="categories"><a href="/categories/C/">C++</a> / <a href="/categories/C/Sort/">Sort</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-07-22T16:00:00.000Z">2020-07-23</time></p><p class="title"><a href="/2020/07/23/%E8%80%83%E7%A0%94%E8%8B%B1%E8%AF%AD%E5%B0%8F%E4%BD%9C%E6%96%87/">考研英语小作文</a></p><p class="categories"><a href="/categories/%E8%80%83%E7%A0%94/">考研</a> / <a href="/categories/%E8%80%83%E7%A0%94/%E8%8B%B1%E8%AF%AD/">英语</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-06-09T14:46:57.506Z">2020-06-09</time></p><p class="title"><a href="/2020/06/09/%E4%B8%AD%E8%AF%91%E8%8B%B1%E9%87%8D%E8%A6%81%E5%8D%95%E8%AF%8D/"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-05-23T16:00:00.000Z">2020-05-24</time></p><p class="title"><a href="/2020/05/24/%E8%80%83%E7%A0%94%E8%AF%8D%E6%B1%87%E7%B2%BE%E8%AE%B2/">考研词汇精讲</a></p><p class="categories"><a href="/categories/%E8%80%83%E7%A0%94/">考研</a> / <a href="/categories/%E8%80%83%E7%A0%94/%E8%8B%B1%E8%AF%AD/">英语</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Blog of KrisHugo" height="28"></a><p class="is-size-7"><span>&copy; 2021 Kris Hugo</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>